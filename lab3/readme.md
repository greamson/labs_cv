## Цель работы:
Научиться создавать простые системы классификации изображений на основе сверточных нейронных сетей.

## Ход работы

Лабораторная работа выполнялась в среде Google Colab.
Изображения были получены с помощью парсинга. Была выбрана библиотека icrawler.

### ResNet50
По мере углубления нейронной сети градиенты функции потерь начинают уменьшаться до нуля, и поэтому веса не обновляются. 
Эта проблема известна как проблема исчезающего градиента. ResNet по существу решил эту проблему, используя пропущенные 
соединения.  
ResNet-50 — это сверточная нейронная сеть с глубиной 50 слоев. Вы можете загрузить предварительно обученную версию сети, 
обученную на ImageNet. Cеть может классифицировать изображения по 1000 категориям объектов.  
Мы будем использовать torchvision библиотеку, которая не только обеспечивает быстрый доступ к сотням наборов данных 
компьютерного зрения, но также предлагает простые и интуитивно понятные методы их предварительной обработки/преобразования, 
чтобы они были готовы к моделированию.
Так как сеть является предобученной, то стоит упомянуть, на чем она обучалась.  
ImageNet — это набор данных миллионов помеченных изображений с высоким разрешением, относящихся примерно к 22 тысячам категорий. 
Изображения были собраны из Интернета и помечены людьми с помощью краудсорсинга. 

### AlexNet
AlexNet — это глубокая сверточная нейронная сеть, которая была первоначально разработана Алексом Крижевским и его 
коллегами еще в 2012 году. Она была разработана для классификации изображений для конкурса ImageNet LSVRC-2010, где она 
достигла самых современных результатов. 
Для работы был использован модуль torchvision.

### GoogLeNet
GoogLeNet основан на выяснении того, как можно аппроксимировать оптимальную локальную разреженную структуру в сети сверточного 
зрения и покрыть ее легко доступными плотными компонентами.  
Они ограничены размерами фильтров 1x1, 3x3 и 5x5 (это решение было основано скорее на удобстве, чем на 
необходимости). Это также означает, что предлагаемая архитектура представляет собой комбинацию всех этих слоев с их банками 
выходных фильтров, объединенных в один выходной вектор, формирующий вход следующего этапа. Добавление альтернативного 
параллельного пути объединения на каждом таком этапе должно иметь дополнительный положительный эффект.  

- Все свертки, в том числе и внутри модулей Inception, использовали выпрямленную линейную активацию.
- Размер рецептивного поля в этой сети 224x224 RGB с нормализацией.
- «Уменьшение # 3x3» и «Уменьшение # 5x5» означает количество фильтров 1x1 в слое сокращения до свертки 3x3 и 5x5.
- Можно увидеть количество 1x1 в проекционном слое после встроенного максимального объединения в столбце pool proj.
- Все эти уровни редукции/проекции также использовали активацию ReLU.
- Широкие остаточные сети — это вариант ResNet, в котором уменьшается глубина и увеличивается ширина остаточных сетей. 
  Это достигается за счет использования широких остаточных блоков. Широкие остаточные сети просто имеют большее количество 
  каналов по сравнению с ResNet. В остальном архитектура та же. 
- Для обучения использовали наборы CIFAR-10, CIFAR-100 и SVHN. Оригинальный ResNet использует ImageNet.


## Результаты экспериментов
| Критерий | ResNet50 | AlexNet | GoogLeNet |
| -------- |----------| ----- | ---------- |
|Top-1 accuracy|0.78|0.72|0.92|
|Top-5 accuracy|1.0|0.94|1.0|
|Время выполнения на GPU (сек.)|39.83|14.36|5.04|
|Потребляемая память (байт)|1522.20|4068.94|4123.53|

## Выводы
Лидером по точности и скорости в данной работе является GoogLeNet, однако, этот метод задействовал почти в 2,5 раза 
больше памяти, чем ResNet50, показатели которого, лишь немного уступают. 

### Список использованной литературы
- https://pytorch.org/vision/stable/models.html
- https://pytorch.org/hub/pytorch_vision_alexnet/
- https://pytorch.org/hub/pytorch_vision_googlenet/